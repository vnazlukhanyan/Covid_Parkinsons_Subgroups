{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import Window\n",
    "from pyspark.sql import types \n",
    "from pyspark.sql.functions import *\n",
    "\n",
    "\n",
    "ss = SparkSession.builder.config('spark.driver.extraClassPath',\n",
    "                                 'postgresql-42.2.18.jar') \\\n",
    "                        .config('spark.driver.memory',\n",
    "                                 '8g') \\\n",
    "                        .config('spark.executor.memory',\n",
    "                                 '8g') \\\n",
    "                         .getOrCreate()\n",
    "sc = ss.sparkContext"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preparation for Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Goals:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. **Join mood, covid, and about on date and id** (Christabelle)\n",
    "    - Rename duplicate columns for each table\n",
    "    - Join mood and covid by id and date\n",
    "    - Grab only the most recent entry for every id in the about df\n",
    "    - Join mood/covid df with the unique about df by id\n",
    "    - Drop renamed duplicate columns and rename to original columns\n",
    "    \n",
    "2. **Drop columns with null values** \n",
    "\n",
    "3. **Remove 'prefer not to answer'**: (Daniel)\n",
    "    - Drop mood rows that contain 'prefer not to answer'\n",
    "            - Mood PNA: 3\n",
    "    - Drop about rows that contain 'prefer not to answer'\n",
    "            - Education PNA: 9\n",
    "            - Income PNA: 7\n",
    "            - Employment PNA: 5\n",
    "            - Veteran PNA: 3\n",
    "            - Research PNA: 3\n",
    "    - Drop columns that correspond to PNA: 'HeightPNA', 'WeightPNA', 'RacePNA','EthnPNA'\n",
    "    \n",
    "    \n",
    "4. **Group mood columns together into separate categories** (Victor)\n",
    "    - Return the sum of the positive mood columns into new column 'positive' and drop the old columns \n",
    "    - Return the sum of the negative mood columns into new column 'negative' and drop the old columns \n",
    "    - Return the sum of the lonely mood columns into new column 'lonely' and drop the old columns \n",
    "    - Return the sum of the energy mood columns into new column 'energy' and drop the old columns \n",
    "    \n",
    "    \n",
    "5. **Principal Component Analysis** (Christabelle, Daniel, Victor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # load data locally\n",
    "# df_covid = ss.read.csv(\"FoxInsight/COVID_19_Experience.csv\", header=True).cache()\n",
    "# df_about = ss.read.csv(\"FoxInsight/About.csv\", header=True).cache()\n",
    "# df_mood = ss.read.csv(\"FoxInsight/Mood.csv\", header=True).cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Join mood, covid, and about on date and id: (Christabelle)\n",
    "    - Rename duplicate columns for each table\n",
    "    - Join mood and covid by id and date\n",
    "    - Grab only the most recent entry for every id in the about df\n",
    "    - Join mood/covid df with the unique about df by id\n",
    "    - Drop renamed duplicate columns and rename to original columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rename_duplicate(df, name):\n",
    "    return df.withColumnRenamed('fox_insight_id', f'fox_insight_id_{name}')\\\n",
    "              .withColumnRenamed('days_elapsed', f'days_elapsed_{name}')\\\n",
    "              .withColumnRenamed('days_acquired', f'days_acquired_{name}')\\\n",
    "              .withColumnRenamed('schedule_of_activities', f'schedule_of_activities_{name}')\\\n",
    "              .withColumnRenamed('age', f'age_{name}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# renaming duplicate columns\n",
    "df_mood = rename_duplicate(df_mood, 'm')\n",
    "df_about = rename_duplicate(df_about, 'a')\n",
    "df_covid = rename_duplicate(df_covid, 'c')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merging mood and covid by id and date\n",
    "df_merge_mc = df_mood.join(df_covid, \n",
    "                         (df_covid.fox_insight_id_c == df_mood.fox_insight_id_m) \\\n",
    "                          & (df_covid.days_elapsed_c == df_mood.days_elapsed_m), how = 'inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_about.select('fox_insight_id_a').show(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# return unique rows in about df\n",
    "df_about = df_about.withColumn(\"days_elapsed\", df_about[\"days_elapsed_a\"].cast(\"int\"))\n",
    "df_about_max = df_about.select('*',max('days_elapsed').over(Window.partitionBy('fox_insight_id_a')).alias('max_days'))\n",
    "df_about_unique = df_about_max.filter(df_about_max.days_elapsed == df_about_max.max_days)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# count should be 50565\n",
    "df_about_unique.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_about_unique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_about_unique.select('fox_insight_id_a').show(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_about_unique.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merging mood/covid and about by id\n",
    "df_merge_mca = df_merge_mc.join(df_about_unique, df_merge_mc.fox_insight_id_c == \\\n",
    "                               df_about_unique.fox_insight_id_a, how = 'inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# count should be 245\n",
    "df_merge_mca.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop duplicate columns and rename\n",
    "df_merge_mca = df_merge_mca.drop('fox_insight_id_c', 'fox_insight_id_m', 'days_elapsed_m', 'days_elapsed_c')\n",
    "df_merge_mca = df_merge_mca.drop('age_c', 'age_m', 'days_acquired_c', 'days_acquired_m', 'days_elapsed_a')\n",
    "df_merge_mca = df_merge_mca.withColumnRenamed('fox_insight_id_a', 'fox_insight_id')\n",
    "df_merge_mca = df_merge_mca.withColumnRenamed('days_elapsed_a', 'days_elapsed')\n",
    "df_merge_mca = df_merge_mca.withColumnRenamed('days_acquired_a', 'days_acquired')\n",
    "df_merge_mca = df_merge_mca.withColumnRenamed('age_a', 'age')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_merge_mca.select('fox_insight_id').show(15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Drop columns with null values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def drop_null_columns(df):\n",
    "    \"\"\"\n",
    "    This function drops all columns which contain null values.\n",
    "    :param df: A PySpark DataFrame\n",
    "    \"\"\"\n",
    "    null_counts = df.select([count(when(col(c).isNull(), c)).alias(c) for c in df.columns]).collect()[0].asDict()\n",
    "    to_drop = [k for k, v in null_counts.items() if v > 0]\n",
    "    df = df.drop(*to_drop)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = drop_null_columns(df_merge_mca)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove: COVSrcInfo\n",
    "df = df.drop('COVSrcInfo').cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remove 'Prefer Not to Answer': Daniel Carrera\n",
    "1. Drop rows that contain 'prefer not to answer' in the mood columns\n",
    "            - Mood PNA: 3\n",
    "2. Drop about rows that contain 'prefer not to answer' in the following columns:\n",
    "            - Education PNA: 9\n",
    "            - Income PNA: 7\n",
    "            - Employment PNA: 5\n",
    "            - Veteran PNA: 3\n",
    "            - Research PNA: 3\n",
    "3. Drop columns that correspond to PNA: 'HeightPNA', 'WeightPNA', 'RacePNA','EthnPNA'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop rows that contain 'prefer not to answer' on the mood columns \n",
    "def drop_mood_rows_w3(df):\n",
    "    cols = list(df_mood.columns)\n",
    "    mood_cols = [col for col in cols if col.startswith('Mood')]\n",
    "    \n",
    "    for col in mood_cols:\n",
    "        df = df.where(f'{col} != {3}')\n",
    "        \n",
    "    return df    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show values before removing rows with 3\n",
    "df.select('MoodInterest').distinct().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = drop_mood_rows_w3(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show values after removing rows with 3\n",
    "df.select('MoodInterest').distinct().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop about rows that contain 'prefer not to answer' \n",
    "\n",
    "columns = {'Education':9, 'Income':7, 'Employment':5, 'Veteran':3, 'Research':3}\n",
    "def drop_pna_rows(df, columns):\n",
    "    for k,v in columns.items():\n",
    "        df = df.where(f'{k} != {v}')\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in columns.keys():\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = drop_pna_rows(df, columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in columns.keys():\n",
    "    df.select(col).distinct().orderBy(col).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop columns that correspond to PNA: 'HeightPNA', 'WeightPNA', 'RacePNA','EthnPNA'\n",
    "df = df.drop('HeightPNA', 'WeightPNA', 'RacePNA', 'EthnPNA') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Group mood columns together into separate categories (Victor)\n",
    "1. Return the sum of the positive mood columns into new column 'positive' and drop the old columns \n",
    "2. Return the sum of the negative mood columns into new column 'negative' and drop the old columns \n",
    "3. Return the sum of the lonely mood columns into new column 'lonely' and drop the old columns \n",
    "4. Return the sum of the energy mood columns into new column 'energy' and drop the old columns "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "positive= ['MoodSatis',  'MoodSpirits','MoodHappy','MoodAlive'] \n",
    "\n",
    "lonely = ['MoodHome', 'MoodInterest', 'MoodEmpty', 'MoodBored', 'MoodBetter']\n",
    "\n",
    "negative = ['MoodHome', 'MoodInterest', 'MoodBored', 'MoodAfraid', 'MoodHelp',\n",
    "            'MoodMemory',  'MoodBetter', 'MoodWorth','MoodHopeless', 'MoodEmpty']\n",
    "\n",
    "energy = ['MoodEnergy']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.select('MoodHappy').printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cast each column from string to int\n",
    "for col in [col for col in df.columns if col != 'fox_insight_id' ]: \n",
    "    df = df.withColumn(col, df[col].cast(\"int\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[positive].show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from operator import add\n",
    "from functools import reduce"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sum across rows for moods cast to new column\n",
    "df = df.withColumn('positive', reduce(add, [df[x] for x in positive]))\n",
    "df = df.withColumn('negative', reduce(add, [df[x] for x in negative]))\n",
    "df = df.withColumn('energy', reduce(add, [df[x] for x in energy]))\n",
    "df = df.withColumn('lonely', reduce(add, [df[x] for x in lonely]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in ['positive', 'negative', 'energy', 'lonely']:\n",
    "    df.select(col).show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for category in [positive, negative, energy]:\n",
    "    for col in category:\n",
    "        df = df.drop(col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting Rid of Columns with 1 Distinct Value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# printing distinct count of each column\n",
    "for col in df.columns:\n",
    "    print(f'{col} Count: {df.select(col).distinct().count()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop all columns with 1 distinct value\n",
    "df = df.drop('COVClinicalTrial', 'days_acquired', 'RaceNH', 'EthnCuban')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Principal Component Analysis (Christabelle, Daniel, Victor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import PCA\n",
    "from pyspark.ml.linalg import Vectors  # Pre 2.0 pyspark.mllib.linalg\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "#Kmeans \n",
    "from pyspark.ml.clustering import KMeans\n",
    "# Clustering Evaluator\n",
    "from pyspark.ml.evaluation import ClusteringEvaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merging the data with Vector Assembler.\n",
    "va = VectorAssembler(outputCol=\"features\", inputCols=[col for col in df.columns if col != 'fox_insight_id'])\n",
    "df_va = va.transform(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(k=15, inputCol=\"features\", outputCol=\"pca\")\n",
    "model = pca.fit(df_va.select('features'))\n",
    "df_pca = model.transform(df_va)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans =  KMeans(k = 6, maxIter = 200, tol = 0.1, featuresCol='pca') \n",
    "model = kmeans.fit(df_pca)\n",
    "df_kmeans = model.transform(df_pca)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_kmeans.select('prediction').distinct().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Center of each cluster\n",
    "centers = model.clusterCenters()\n",
    "print(\"Cluster Centers: \")\n",
    "for center in centers:\n",
    "    print(center)\n",
    "    print(center.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluator = ClusteringEvaluator(featuresCol='pca')\n",
    "silhouette = evaluator.evaluate(df_kmeans)\n",
    "print(\"Silhouette with squared euclidean distance = \" + str(silhouette))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The silhouette value is a measure of how similar an object is to its own cluster (cohesion) compared to other clusters (separation). The silhouette ranges from âˆ’1 to +1, where a high value indicates that the object is well matched to its own cluster and poorly matched to neighboring clusters. If most objects have a high value, then the clustering configuration is appropriate. If many points have a low or negative value, then the clustering configuration may have too many or too few clusters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "silhouette = []\n",
    "for i in range(2,15):\n",
    "    kmeans =  KMeans(k = i, maxIter = 200, tol = 0.1) \n",
    "    model = kmeans.fit(df_pca)\n",
    "    predictions = model.transform(df_pca)\n",
    "    evaluator = ClusteringEvaluator()\n",
    "    silhouette.append(evaluator.evaluate(predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (10,8))\n",
    "plt.plot(range(2,15), silhouette, marker = 'o', linestyle = '--')\n",
    "plt.xlabel('Number of Clusters')\n",
    "plt.ylabel('Silhouette')\n",
    "plt.title('K-means with PCA Clustering')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_kmeans.write.parquet(\"s3://msds694-parkinsons\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data locally\n",
    "df_kmeans = ss.read.parquet(\"df_pca\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.stop()\n",
    "ss.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
